import pandas as pd
from pprint import pprint
import json
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

def extract_ember(rows):

    with open("C:/Users/alex/UMTC/3rd Year/CST 9L/Repo/Xira/ember_dataset/ember/train_features_1.jsonl") as f:
        for line in f:
            sample = json.loads(line);
        
            text_entropy = 0;
            rsrc_entropy = 0;

            #Scanning Entropy
            for sec in sample.get("section", {}).get("sections", []):
                if sec["name"] == ".text":
                    text_entropy = sec.get("entropy", 0)
                elif sec["name"] == ".rsrc":
                    rsrc_entropy = sec.get("entropy", 0)
        
            rows.append({
                "file_size": sample["general"].get("size", 0),
                "virtual_size": sample["general"].get("vsize", 0),
                "imports": sample["general"].get("imports", 0),
                "exports": sample["general"].get("exports", 0),
                "sections": len(sample["section"]["sections"]) if "section" in sample else 0,
                "has_signature": sample["general"].get("has_signature", 0),
                "has_tls": sample["general"].get("has_tls", 0),
                "text_entropy": text_entropy,
                "rsrc_entropy": rsrc_entropy,
                "malicious": sample["label"]
            })
    return rows;

def knn_classifier(features):
    rows = [];
    rows = extract_ember(rows);
    data = pd.DataFrame(rows);
    data = data[data["malicious"] >= 0]
    # print(data.head(10),'\n\n\n');

    # print('Null Values: ', data.isna().sum())

    #Getting features and target
    X = data.drop("malicious", axis = 1)
    y = data["malicious"]

    #Train Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

    #Feature Scaling
    sc = StandardScaler();
    X_train = sc.fit_transform(X_train);
    X_test = sc.transform(X_test);

    #Finding the best k for KNN
    # k_values = [20, 25, 30, 35, 40, 45, 50]
    #
    # for k in k_values:
    #     knn = KNeighborsClassifier(n_neighbors = k, algorithm = 'kd_tree')
    #     scores = cross_val_score(knn, X_train, y_train, cv = 3, scoring = 'accuracy', n_jobs = -1)
    #     print('k: ', k, '      accuracy: ', scores.mean())

    #KNN Model Training
    knn = KNeighborsClassifier(n_neighbors = 25, algorithm = 'kd_tree')
    knn.fit(X_train, y_train)

    #predicting
    feats = [[features["file_size"], 
              features["virtual_size"], 
              features["imports"],
              features["exports"],
              features["sections"],
              features["has_signature"],
              features["has_tls"],
              features["text_entropy"],
              features["rsrc_entropy"]
              ]]
    feat_scaled = sc.transform(feats);
    feat_pred = knn.predict(feat_scaled);
    y_pred = knn.predict(X_test);
    acc = accuracy_score(y_test, y_pred)
    return feat_pred;
